{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "目标： 训练一个机器翻译（Machine Translation），功能很简单，就是将一些 human readable的日期，转成 machine readable的日期，固定格式： 'yyyy-mm-dd'\n",
    "\n",
    "1. 准备训练数据\n",
    "2. 构建模型，重点是 Attention机制。\n",
    "3. 训练模型，观察翻译效果，观察 Attention 矩阵（可视化）\n",
    "\n",
    "\n",
    "流程大致了解。 还有很多问题： 最后的模型训练又是稀奇古怪，没有达到预期效果。哪里出问题了。需要改进"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 准备训练数据\n",
    "\n",
    "- faker，制造一些日期数据 \n",
    "- FORMATS，定义一些日期格式\n",
    "- babel，主要是Internationalization 功能\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 11728.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from faker import Faker\n",
    "from babel.dates import format_date    # babel: the Python Internationalization Library\n",
    "\n",
    "\n",
    "fake = Faker()\n",
    "fake.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "LOCALES = ['en_US', 'zh_CN']\n",
    "\n",
    "def load_date():\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS), \n",
    "                                     # locale=random.choice(LOCALES))\n",
    "                                     locale='en_US')\n",
    "                                     # locale='zh_CN')\n",
    "        human_readable = human_readable.lower().replace(',','')\n",
    "        machine_readable = dt.isoformat()\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt\n",
    "\n",
    "\n",
    "def load_dataset(m):\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "    Tx = 30 \n",
    "\n",
    "    for i in tqdm(range(m)):\n",
    "        h, m, _ = load_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "    \n",
    "    # 建立索引\n",
    "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], \n",
    "                     list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    " \n",
    "    return dataset, human, machine, inv_machine\n",
    "\n",
    "\n",
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.09.70', '1970-09-10'),\n",
       " ('4/28/90', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('tuesday july 8 2008', '2008-07-08'),\n",
       " ('08 sep 1999', '1999-09-08'),\n",
       " ('1 jan 1981', '1981-01-01'),\n",
       " ('monday may 22 1995', '1995-05-22')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据转成 numpy array 格式， \n",
    "\n",
    "X 长度Tx=30，长度不固定，短则用 pad 补齐，长了就截掉。 每个字符 One-hot编码； \n",
    "\n",
    "Y 长度Ty=10固定， 每个字符 One-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# 字符转成索引， 长则截短，短则用pad补\n",
    "def string_to_int(string, length, vocab):\n",
    "    #make lower to standardize\n",
    "    string = string.lower().replace(',','')\n",
    "    \n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "        \n",
    "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "    \n",
    "    if len(string) < length:\n",
    "        rep += [vocab['<pad>']] * (length - len(string))\n",
    "    return rep\n",
    "\n",
    "# 先将字符转索引，再 one-hot\n",
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \n",
    "    X, Y = zip(*dataset)\n",
    "    \n",
    "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
    "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
    "    \n",
    "    # 转成 one-hot\n",
    "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
    "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
    "\n",
    "    return X, np.array(Y), Xoh, Yoh\n",
    "\n",
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 9 may 1998\n",
      "Target date: 1998-05-09\n",
      "\n",
      "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Target after preprocessing (one-hot): [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 构建模型\n",
    "\n",
    "\n",
    "### 2.1 - Attention mechanism\n",
    "\n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "注意几点：\n",
    "1. 两层LSTM， 第一层是Bi-LSTM， 双向的，用于解析整句的意思。输入 $x^{<t>}$ ，输出 $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}] $\n",
    "2. Attention是一层简单的NN。所有Attention实际上重复使用。输入 $(s^{<t-1>}, a^{<t'>}), t' = 1 \\cdots Tx$ ，输出 $\\alpha^{<t, t'>}$，将表示LSTM 的下一个输出 $s^{<t>}$ 对 $a^{<t'>}$ 的“注意力”。 最后计算出 $context^{<t>}$\n",
    "$$context^{<t>} = \\sum_{t' = 0}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "3. 最后一层LSTM，注意一点，就是 $\\hat{y}^{<t>}$ 不作为下一个LSTM单元的输入，因为输出的日期，字符之间是没有关联的。\n",
    "\n",
    "左图，Model使用的Keras Layer：\n",
    "- [Bidirectional](https://keras.io/layers/wrappers/#bidirectional)\n",
    "- [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
    "\n",
    "右图，Attention机制使用的 Keras Layer：\n",
    "\n",
    "- [RepeatVector](https://keras.io/layers/core/#repeatvector)，用于复用 $s^{<t-1>}$\n",
    "- [Concatenate](https://keras.io/layers/merge/#concatenate)，用于拼接 $(s^{<t-1>}, a^{<t'>})$\n",
    "- [Dense](https://keras.io/layers/core/#dense)， 连接层， 作用等同于 $ a = activation(Wx + b) $\n",
    "- [Activation](https://keras.io/layers/core/#activation)， activate函数，没什么好说的\n",
    "- [Dot](https://keras.io/layers/merge/#dot)， Dot计算，用在计算 $context^{<t>}$ 中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Bidirectional, LSTM, RepeatVector,  Concatenate, Dense, Activation, Dot\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "\n",
    "        \n",
    "# attention layers\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)\n",
    "\n",
    "# post LSTM layer\n",
    "n_a = 64\n",
    "n_s = 128\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    # a: (m, Tx, n_a*2)\n",
    "    # s_prev: (m, n_s)\n",
    "    s_prev = repeator(s_prev)    # (m, Tx, n_s)\n",
    "    concat = concatenator([a, s_prev])   # (m, Tx, n_a*2+n_s)\n",
    "    e = densor(concat)      # 维度变化，做一次 ‘relu’， (m, Tx, 1)\n",
    "    alphas = activator(e)   # 在axis=1 上做softmax， (m, Tx, 1)\n",
    "    context = dotor([alphas, a])   # 在axis=1上相乘，相加，得到  (m, 1, n_a*2)\n",
    "    return context\n",
    "\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)    # (m, Tx, n_a*2)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "        context = one_step_attention(a, s)  # (m, 1, n_a*2)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])   # (m, n_s)\n",
    "        out = output_layer(s) # (m, n_s)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model(inputs=[X, s0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 30, 128)      52224       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_8 (RepeatVector)  (None, 30, 128)      0           s0[0][0]                         \n",
      "                                                                 lstm_6[0][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 30, 256)      0           bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[1][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[2][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[3][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[4][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[5][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[6][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[7][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[8][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector_8[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 30, 1)        257         concatenate_8[0][0]              \n",
      "                                                                 concatenate_8[1][0]              \n",
      "                                                                 concatenate_8[2][0]              \n",
      "                                                                 concatenate_8[3][0]              \n",
      "                                                                 concatenate_8[4][0]              \n",
      "                                                                 concatenate_8[5][0]              \n",
      "                                                                 concatenate_8[6][0]              \n",
      "                                                                 concatenate_8[7][0]              \n",
      "                                                                 concatenate_8[8][0]              \n",
      "                                                                 concatenate_8[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_12[0][0]                   \n",
      "                                                                 dense_12[1][0]                   \n",
      "                                                                 dense_12[2][0]                   \n",
      "                                                                 dense_12[3][0]                   \n",
      "                                                                 dense_12[4][0]                   \n",
      "                                                                 dense_12[5][0]                   \n",
      "                                                                 dense_12[6][0]                   \n",
      "                                                                 dense_12[7][0]                   \n",
      "                                                                 dense_12[8][0]                   \n",
      "                                                                 dense_12[9][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_7 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 128), (None, 131584      dot_7[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_7[1][0]                      \n",
      "                                                                 lstm_6[0][0]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "                                                                 dot_7[2][0]                      \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[1][2]                     \n",
      "                                                                 dot_7[3][0]                      \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[2][2]                     \n",
      "                                                                 dot_7[4][0]                      \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[3][2]                     \n",
      "                                                                 dot_7[5][0]                      \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[4][2]                     \n",
      "                                                                 dot_7[6][0]                      \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[5][2]                     \n",
      "                                                                 dot_7[7][0]                      \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[6][2]                     \n",
      "                                                                 dot_7[8][0]                      \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[7][2]                     \n",
      "                                                                 dot_7[9][0]                      \n",
      "                                                                 lstm_6[8][0]                     \n",
      "                                                                 lstm_6[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 11)           1419        lstm_6[0][0]                     \n",
      "                                                                 lstm_6[1][0]                     \n",
      "                                                                 lstm_6[2][0]                     \n",
      "                                                                 lstm_6[3][0]                     \n",
      "                                                                 lstm_6[4][0]                     \n",
      "                                                                 lstm_6[5][0]                     \n",
      "                                                                 lstm_6[6][0]                     \n",
      "                                                                 lstm_6[7][0]                     \n",
      "                                                                 lstm_6[8][0]                     \n",
      "                                                                 lstm_6[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 185,484\n",
      "Trainable params: 185,484\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 23s 2ms/step - loss: 0.5770 - dense_13_loss_1: 0.0078 - dense_13_loss_2: 0.0038 - dense_13_loss_3: 0.0401 - dense_13_loss_4: 0.0375 - dense_13_loss_5: 4.2189e-04 - dense_13_loss_6: 0.0285 - dense_13_loss_7: 0.1516 - dense_13_loss_8: 7.0598e-04 - dense_13_loss_9: 0.1129 - dense_13_loss_10: 0.1937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x138c69a90>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))\n",
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training you can see the loss as well as the accuracy on each of the 10 positions of the output. The table below gives you an example of what the accuracies could be if the batch had 2 examples: \n",
    "\n",
    "<img src=\"images/table.png\" style=\"width:700;height:200px;\"> <br>\n",
    "<caption><center>Thus, `dense_2_acc_8: 0.89` means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. </center></caption>\n",
    "\n",
    "\n",
    "We have run this model for longer, and saved the weights. Run the next cell to load our weights. (By training a model for several minutes, you should be able to obtain a model of similar accuracy, but loading our model will save you time.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 19872-1222\n",
      "source: 5 April 09\n",
      "output: 1987-00322\n",
      "source: 21th of August 2016\n",
      "output: 1977-10-14\n",
      "source: Tue 10 Jul 2007\n",
      "output: 1974-03-23\n",
      "source: Saturday May 9 2018\n",
      "output: 1971-10-11\n",
      "source: March 3 2001\n",
      "output: 19874-2444\n",
      "source: March 3rd 2001\n",
      "output: 1977-04424\n",
      "source: 1 March 2001\n",
      "output: 19874-4421\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    source = source.reshape(1, Tx, -1)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Visualizing Attention (Optional / Ungraded)\n",
    "\n",
    "Since the problem has a fixed output length of 10, it is also possible to carry out this task using 10 different softmax units to generate the 10 characters of the output. But one advantage of the attention model is that each part of the output (say the month) knows it needs to depend only on a small part of the input (the characters in the input giving the month). We can  visualize what part of the output is looking at what part of the input.\n",
    "\n",
    "Consider the task of translating \"Saturday 9 May 2018\" to \"2018-05-09\". If we visualize the computed $\\alpha^{\\langle t, t' \\rangle}$ we get this: \n",
    "\n",
    "<img src=\"images/date_attention.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 8**: Full Attention Map</center></caption>\n",
    "\n",
    "Notice how the output ignores the \"Saturday\" portion of the input. None of the output timesteps are paying much attention to that portion of the input. We see also that 9 has been translated as 09 and May has been correctly translated into 05, with the output paying attention to the parts of the input it needs to to make the translation. The year mostly requires it to pay attention to the input's \"18\" in order to generate \"2018.\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Getting the activations from the network\n",
    "\n",
    "Lets now visualize the attention values in your network. We'll propagate an example through the network, then visualize the values of $\\alpha^{\\langle t, t' \\rangle}$. \n",
    "\n",
    "To figure out where the attention values are located, let's start by printing a summary of the model ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate through the output of `model.summary()` above. You can see that the layer named `attention_weights` outputs the `alphas` of shape (m, 30, 1) before `dot_2` computes the context vector for every time step $t = 0, \\ldots, T_y-1$. Lets get the activations from this layer.\n",
    "\n",
    "The function `attention_map()` pulls out the attention values from your model and plots them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday April 08 1993\", num = 6, n_s = 128)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
