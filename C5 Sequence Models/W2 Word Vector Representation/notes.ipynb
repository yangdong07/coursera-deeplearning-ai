{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on word vectors\n",
    "\n",
    "1. 认识 word embeddings： word vectors 长啥样\n",
    "2. cosine similarity\n",
    "3. word analogy 单词类比\n",
    "4. Debiasing word vectors， 去除性别/种族偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. word embeddings\n",
    "\n",
    "使用已经训练好的 word embeddings： glove.6B.50d.txt\n",
    "\n",
    "下载地址： https://nlp.stanford.edu/projects/glove/  ， [glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "\n",
    "**注意**： jupyter notebook的默认IO速度忒慢， 1,000,000 bytes/sec， 也就是1M/s 的速度。 这个文件有100多M，等的蛋疼\n",
    "\n",
    "启动的时候设置： `jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000`  ， 10G/s 应该不是瓶颈了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(line[0])\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map\n",
    "\n",
    "\n",
    "words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "the\n",
      "[  4.18000000e-01   2.49680000e-01  -4.12420000e-01   1.21700000e-01\n",
      "   3.45270000e-01  -4.44570000e-02  -4.96880000e-01  -1.78620000e-01\n",
      "  -6.60230000e-04  -6.56600000e-01   2.78430000e-01  -1.47670000e-01\n",
      "  -5.56770000e-01   1.46580000e-01  -9.50950000e-03   1.16580000e-02\n",
      "   1.02040000e-01  -1.27920000e-01  -8.44300000e-01  -1.21810000e-01\n",
      "  -1.68010000e-02  -3.32790000e-01  -1.55200000e-01  -2.31310000e-01\n",
      "  -1.91810000e-01  -1.88230000e+00  -7.67460000e-01   9.90510000e-02\n",
      "  -4.21250000e-01  -1.95260000e-01   4.00710000e+00  -1.85940000e-01\n",
      "  -5.22870000e-01  -3.16810000e-01   5.92130000e-04   7.44490000e-03\n",
      "   1.77780000e-01  -1.58970000e-01   1.20410000e-02  -5.42230000e-02\n",
      "  -2.98710000e-01  -1.57490000e-01  -3.47580000e-01  -4.56370000e-02\n",
      "  -4.42510000e-01   1.87850000e-01   2.78490000e-03  -1.84110000e-01\n",
      "  -1.15140000e-01  -7.85810000e-01]\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "for word in word_to_vec_map:\n",
    "    print(word)\n",
    "    print(word_to_vec_map[word])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Cosine similarity\n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n",
    "\n",
    "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
    "<caption><center> **Figure 1**: The cosine of the angle between two vectors is a measure of how similar they are</center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine #  cosine = 1 - cosine_similarity\n",
    "\n",
    "def cosine_similarity(u, v):   \n",
    "    distance = 0.0\n",
    "    dot = np.dot(u.T, v)\n",
    "    norm_u = np.linalg.norm(u, 2)\n",
    "    norm_v = np.linalg.norm(v, 2)\n",
    "    cosine_similarity = dot / norm_u / norm_v\n",
    "    \n",
    "    return cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(father, mother) =  0.890903844289\n",
      "cosine_similarity(ball, crocodile) =  0.274392462614\n",
      "cosine_similarity(france - paris, rome - italy) =  -0.675147930817\n"
     ]
    }
   ],
   "source": [
    "father = word_to_vec_map[\"father\"]\n",
    "mother = word_to_vec_map[\"mother\"]\n",
    "ball = word_to_vec_map[\"ball\"]\n",
    "crocodile = word_to_vec_map[\"crocodile\"]\n",
    "france = word_to_vec_map[\"france\"]\n",
    "italy = word_to_vec_map[\"italy\"]\n",
    "paris = word_to_vec_map[\"paris\"]\n",
    "rome = word_to_vec_map[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(father, mother)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.890903844289\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(ball, crocodile)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.274392462614\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(france - paris, rome - italy)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         -0.675147930817\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Word analogy task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    # convert words to lower case\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    \n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in word_to_vec_map.keys():        \n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
    "        \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> larger\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **italy -> italian** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         spain -> spanish\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **india -> delhi** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         japan -> tokyo\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **man -> woman ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         boy -> girl\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **small -> smaller ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         large -> larger\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man -> woman :: king -> princess\n"
     ]
    }
   ],
   "source": [
    "triad = ('man', 'woman', 'king')\n",
    "print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man -> woman :: computer -> 816-822-8448\n"
     ]
    }
   ],
   "source": [
    "triad = ('man', 'woman', 'computer')\n",
    "print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good -> bad :: money -> subprime\n"
     ]
    }
   ],
   "source": [
    "triad = ('good', 'bad', 'money')\n",
    "print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lovely -> money :: evil -> funds\n"
     ]
    }
   ],
   "source": [
    "triad = ('lovely', 'money', 'evil')\n",
    "print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yang -> dong :: macbook -> vlcc\n"
     ]
    }
   ],
   "source": [
    "triad = ('yang', 'dong', 'macbook')\n",
    "print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Debiasing word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165\n",
      " -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824\n",
      "  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122\n",
      "  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445\n",
      "  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115\n",
      " -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957\n",
      " -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426\n",
      "  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455\n",
      " -0.04371     0.01258   ]\n"
     ]
    }
   ],
   "source": [
    "bias_woman_man = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "print(bias_woman_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些名字是具有性别特征的，可以从名字与 `bias_woman_man` 矢量（轴）上的夹角（cosine）可以看出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with constructed vector:\n",
      "john -0.23163356146\n",
      "marie 0.315597935396\n",
      "sophie 0.318687898594\n",
      "ronaldo -0.312447968503\n",
      "priya 0.17632041839\n",
      "rahul -0.169154710392\n",
      "danielle 0.243932992163\n",
      "reza -0.079304296722\n",
      "katy 0.283106865957\n",
      "yasmin 0.233138577679\n"
     ]
    }
   ],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], bias_woman_man))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "不幸？的是，职业也有性别特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other words and their similarities:\n",
      "    lipstick      0.2769\n",
      "        guns     -0.1888\n",
      "     science     -0.0608\n",
      "        arts      0.0082\n",
      "  literature      0.0647\n",
      "     warrior     -0.2092\n",
      "      doctor      0.1190\n",
      "        tree     -0.0709\n",
      "receptionist      0.3308\n",
      "  technology     -0.1319\n",
      "     fashion      0.0356\n",
      "     teacher      0.1792\n",
      "    engineer     -0.0804\n",
      "       pilot      0.0011\n",
      "    computer     -0.1033\n",
      "      singer      0.1850\n"
     ]
    }
   ],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print ('%12s  %10.4f' % (w, cosine_similarity(word_to_vec_map[w], bias_woman_man)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 - Neutralize bias for non-gender specific words \n",
    "\n",
    "\n",
    "<img src=\"images/neutral.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> **Figure 2**: The word vector for \"receptionist\" represented before and after applying the neutralize operation. </center></caption>\n",
    "\n",
    "\n",
    "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g\\tag{2}$$\n",
    "$$e^{debiased} = e - e^{bias\\_component}\\tag{3}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neutralize(word, bias_vector, word_to_vec_map):\n",
    "    bias_unit_vector = bias_vector / np.linalg.norm(bias_vector, 2)\n",
    "    e = word_to_vec_map[word]\n",
    "    e_biascomponent = np.dot(e, bias_unit_vector) * bias_unit_vector\n",
    "    e_debiased = e - e_biascomponent\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = \"receptionist\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], g))\n",
    "\n",
    "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: The second result is essentially 0, up to numerical roundof (on the order of $10^{-17}$).\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine similarity between receptionist and g, before neutralizing:** :\n",
    "        </td>\n",
    "        <td>\n",
    "         0.330779417506\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine similarity between receptionist and g, after neutralizing:** :\n",
    "        </td>\n",
    "        <td>\n",
    "         -3.26732746085e-17\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Equalization algorithm for gender-specific words\n",
    "\n",
    "<img src=\"images/equalize10.png\" style=\"width:800px;height:400px;\">\n",
    "\n",
    "\n",
    "The derivation of the linear algebra to do this is a bit more complex. (See Bolukbasi et al., 2016 for details.) But the key equations are: \n",
    "\n",
    "$$ \\mu = \\frac{e_{w1} + e_{w2}}{2}\\tag{4}$$ \n",
    "\n",
    "$$ \\mu_{B} = (\\mu \\cdot \\text{bias_unit_vector}) *\\text{bias_unit_vector}  \\tag{5}$$ \n",
    "\n",
    "$$\\mu_{\\perp} = \\mu - \\mu_{B} \\tag{6}$$\n",
    "\n",
    "$$ e_{w1B} = (e_{w1} \\cdot \\text{bias_unit_vector}) *\\text{bias_unit_vector}\n",
    "\\tag{7}$$ \n",
    "$$ e_{w2B} = (e_{w2} \\cdot \\text{bias_unit_vector}) *\\text{bias_unit_vector}\n",
    "\\tag{8}$$\n",
    "\n",
    "\n",
    "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {||e_{w1B} - \\mu_B||} \\tag{9}$$\n",
    "\n",
    "\n",
    "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {||e_{w2B} - \\mu_B||} \\tag{10}$$\n",
    "\n",
    "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp} \\tag{11}$$\n",
    "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp} \\tag{12}$$\n",
    "\n",
    "\n",
    "**Exercise**: Implement the function below. Use the equations above to get the final equalized version of the pair of words. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equalize(pair, bias_vector, word_to_vec_map):\n",
    "    \n",
    "    # Step 1: Select word vector representation of \"word\". Use word_to_vec_map. (≈ 2 lines)\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "    \n",
    "    # Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)\n",
    "    mu = (e_w1 + e_w2) / 2\n",
    "    bias_unit_vector = bias_vector / np.linalg.norm(bias_vector, 2)\n",
    "\n",
    "    # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)\n",
    "    mu_B = np.dot(mu, bias_unit_vector) * bias_unit_vector\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)\n",
    "    e_w1B = np.dot(e_w1, bias_unit_vector) * bias_unit_vector\n",
    "    e_w2B = np.dot(e_w2, bias_unit_vector) * bias_unit_vector\n",
    "        \n",
    "    # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)\n",
    "    factor = np.sqrt(np.abs(1 - np.sum(mu_orth * mu_orth)))\n",
    "    corrected_e_w1B = factor / np.linalg.norm(e_w1B - mu_B, 2) * (e_w1B - mu_B)\n",
    "    corrected_e_w2B = factor / np.linalg.norm(e_w2B - mu_B, 2) * (e_w2B - mu_B)\n",
    "\n",
    "    # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)\n",
    "    e1 = mu_orth + corrected_e_w1B\n",
    "    e2 = mu_orth + corrected_e_w2B\n",
    "    \n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "\n",
    "print(\"!!!!!!bias = woman - man!!!!!!!!\")\n",
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"boy\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"boy\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"girl\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"girl\"], g))\n",
    "print()\n",
    "e1, e2 = equalize((\"boy\", \"girl\"), g, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g1 = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "g2 = word_to_vec_map['mother'] - word_to_vec_map['father']\n",
    "g3 = word_to_vec_map['girl'] - word_to_vec_map['boy']\n",
    "\n",
    "print(cosine_similarity(g1, g2))\n",
    "print(cosine_similarity(g1, g3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "- The debiasing algorithm is from Bolukbasi et al., 2016, [Man is to Computer Programmer as Woman is to\n",
    "Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n",
    "- The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "8hb5s",
   "launcher_item_id": "5NrJ6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
