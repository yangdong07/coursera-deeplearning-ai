{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN exercises\n",
    "\n",
    "\n",
    "ç›®æ ‡ï¼š\n",
    "\n",
    "1. Character Language Model,  Basic RNN\n",
    "    1. word representation\n",
    "    2. character language model\n",
    "    3. sample\n",
    "    4. train\n",
    "    5. summary\n",
    "2. Writing like Shakespeare, LSTM RNN\n",
    "    1. Model\n",
    "    2. Train\n",
    "    3. Generate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 å–åå­—\n",
    "\n",
    "ç»™å®šä¸€ä¸ªnameåˆ—è¡¨ï¼ˆè®­ç»ƒé›†ï¼‰ï¼Œè®­ç»ƒRNNæ¨¡å‹ï¼Œå¾—åˆ°ä¸€ä¸ª Character Language Modelï¼ˆæ¦‚ç‡æ¨¡å‹ï¼‰ã€‚ ç„¶åé€šè¿‡è¿™ä¸ªModel Sampleå‡ºä¸€äº›nameã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# æ•°æ®é›†ï¼š list of names\n",
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "# ç»Ÿè®¡å­—ç¬¦ï¼Œå­—ç¬¦å­—å…¸\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "<img src=\"images/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> **Figure 1**: Recurrent Neural Network, similar to what you had built in the previous notebook \"Building a RNN - Step by Step\".  </center></caption>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 2**: In this picture, we assume the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step, and have the network then sample one character at a time. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnCell(object):\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.graidents = {}\n",
    "        self.x = None\n",
    "        self.ai = None\n",
    "        self.y = None\n",
    "        self.ao = None\n",
    "        self.dy = None\n",
    "        \n",
    "    def forward_pass(self, x, ai):\n",
    "        Wax, Waa, ba, Wya, by = [self.parameters[k] for k in ['Wax', 'Waa', 'ba', 'Wya', 'by']]\n",
    "        self.x = x\n",
    "        self.ai = ai\n",
    "        self.ao = np.tanh(np.dot(Wax, x) + np.dot(Waa, ai) + ba)\n",
    "        self.y = softmax(np.dot(Wya, self.ao) + by)\n",
    "        return self.ao\n",
    "    \n",
    "    def backward_pass(self, dao):\n",
    "        Wax, Waa, Wya = [self.parameters[k] for k in ['Wax', 'Waa', 'Wya']]\n",
    "        dy = self.dy\n",
    "        \n",
    "        dWya = np.dot(dy, self.ao.T)\n",
    "        dby = dy\n",
    "        da = dao + np.dot(Wya.T, dy)\n",
    "        dz = da * (1 - self.ao * self.ao)  # a = tanh(z), dz = da * (1 - a * a)\n",
    "        dWaa = np.dot(dz, self.ai.T)\n",
    "        dWax = np.dot(dz, self.x.T)\n",
    "        dba = dz\n",
    "        dai = np.dot(Waa.T, dz)\n",
    "        gradients = {'Wax': dWax, 'Waa': dWaa, 'ba': dba, 'Wya': dWya, 'by': dby}\n",
    "        return gradients, dai        \n",
    "\n",
    "\n",
    "class CharacterLanguageModel():\n",
    "    def __init__(self, n_a=50, vocab_size=27):\n",
    "        self.n_x = vocab_size\n",
    "        self.n_y = vocab_size   # T_y = T_x, n_y = n_x\n",
    "        self.n_a = n_a\n",
    "        self.initialize_parameters(n_a, vocab_size, vocab_size)\n",
    "        \n",
    "    def initialize_parameters(self, n_a, n_x, n_y, seed=1):\n",
    "        np.random.seed(seed)\n",
    "        Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "        Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "        Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "        b = np.zeros((n_a, 1)) # hidden bias\n",
    "        by = np.zeros((n_y, 1)) # output bias\n",
    "\n",
    "        self.parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": b,\"by\": by}\n",
    "        \n",
    "    def run_forward(self, X, a0):\n",
    "        T_x = len(X)\n",
    "        loss = 0.0\n",
    "        cells = []\n",
    "        \n",
    "        x = np.zeros((self.n_x, 1))\n",
    "        ai = np.copy(a0)\n",
    "        for t in range(T_x):\n",
    "            cell = RnnCell(self.parameters)\n",
    "            ai = cell.forward_pass(x, ai)\n",
    "            \n",
    "            # loss and dy\n",
    "            loss -= np.log(cell.y[X[t], 0])\n",
    "            cell.dy = cell.y.copy()\n",
    "            cell.dy[X[t]] -= 1\n",
    "            \n",
    "            cells.append(cell)\n",
    "            # for next\n",
    "            x = np.zeros((self.n_x, 1))\n",
    "            x[X[t]] = 1\n",
    "        \n",
    "#         print(cells[-1].ao[:5])\n",
    "        return loss, cells\n",
    "    \n",
    "    def run_backwards(self, cells):\n",
    "        dao = np.zeros((self.n_a, 1))\n",
    "        gradients = {}\n",
    "        for k in self.parameters:\n",
    "            gradients[k] = np.zeros_like(self.parameters[k])\n",
    "        for cell in reversed(cells):\n",
    "            grad, dao = cell.backward_pass(dao) \n",
    "            for k in grad:\n",
    "                gradients[k] += grad[k]\n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients, learning_rate):        \n",
    "        # clip\n",
    "        for gradient in gradients.values():\n",
    "            np.clip(gradient, -1, 1, out=gradient)\n",
    "            \n",
    "        for k in self.parameters:\n",
    "            self.parameters[k] += -learning_rate * gradients[k]\n",
    "     \n",
    "    def optimize(self, X, a0, learning_rate=0.01):\n",
    "        # X: list of character index        \n",
    "        T_x = len(X)\n",
    "        loss, cells = self.run_forward(X, a0)\n",
    "        gradients = self.run_backwards(cells)\n",
    "        self.update_parameters(gradients, learning_rate)\n",
    "        return loss, gradients, cells[-1].ao\n",
    "        \n",
    "    def train(self, data, ix_to_char, char_to_ix, num_iterations=35000, learning_rate=0.01):\n",
    "        # data:  list of lines, from f.readlines()\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(data)\n",
    "        \n",
    "        # Optimization loop\n",
    "        loss = get_initial_loss(self.n_x, 7) #??\n",
    "        a0 = np.zeros((self.n_a, 1))\n",
    "        for j in range(num_iterations):\n",
    "            index = j % len(data)\n",
    "            X = [char_to_ix[ch] for ch in data[index]]\n",
    "            \n",
    "            curr_loss, gradients, a0 = self.optimize(X, a0, learning_rate=0.01)\n",
    "            loss = smooth(loss, curr_loss)\n",
    "#             if j % 100 == 0:\n",
    "#                 print(curr_loss, loss)      \n",
    "            if j % 2000 == 0:\n",
    "                print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "                seed = 0\n",
    "                for name in range(7):\n",
    "\n",
    "                    # Sample indices and print them\n",
    "                    sampled_indices = self.sample(char_to_ix['\\n'], seed=seed)\n",
    "                    print_sample(sampled_indices, ix_to_char)\n",
    "                    seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "                print('\\n')\n",
    "        return curr_loss\n",
    "    \n",
    "    def sample(self, eos=0, max_len=50, seed=1):\n",
    "        \n",
    "        Wax, Waa, ba, Wya, by = [self.parameters[k] for k in ['Wax', 'Waa', 'ba', 'Wya', 'by']]\n",
    "        n_x, n_a = self.n_x, self.n_a\n",
    "        \n",
    "        indices = []\n",
    "        idx = -1 \n",
    "        counter = 0\n",
    "        \n",
    "        # 1. init x and a_prev\n",
    "        x = np.zeros((n_x, 1))\n",
    "        ai = np.zeros((n_a, 1))\n",
    "        while (idx != eos and counter != max_len):\n",
    "            # 2. calculate\n",
    "            a = np.tanh(np.dot(Waa, ai) + np.dot(Wax, x) + ba)\n",
    "            y = softmax(np.dot(Wya, a) + by)\n",
    "\n",
    "            # 3. Sample the index of a character within the vocabulary from the probability distribution y\n",
    "            np.random.seed(counter + seed) \n",
    "            idx = np.random.choice(n_x, p = y.ravel())\n",
    "            indices.append(idx)\n",
    "            \n",
    "            # 4. next x = y : x<t> = y<t-1>, x : one-hot encoding\n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            \n",
    "            seed += 1\n",
    "            counter +=1\n",
    "            ai = a\n",
    "\n",
    "        if (counter == max_len):\n",
    "            indices.append(eos)\n",
    "        return indices\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "aachenosaurus\n",
      "\n",
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 27.950466\n",
      "\n",
      "Mhytrpdmeromxgortariontoclusuonancesatlasalepdtonp\n",
      "Hledalpsamantisaurus\n",
      "Iwtrpdlerndxhortarinesganusvkecielulen\n",
      "Macalpsamantisaurus\n",
      "Ytrpckgoraurus\n",
      "A\n",
      "Troligoraurus\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.894695\n",
      "\n",
      "Onytos\n",
      "Kledalosaurus\n",
      "Lytosaurus\n",
      "Oma\n",
      "Xtrolonmkaveros\n",
      "Cabasemachus\n",
      "Toraraurus\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.697856\n",
      "\n",
      "Rixtosaurus\n",
      "Nneeaitos\n",
      "Nytosaurus\n",
      "Racalosaurus\n",
      "Xtroionosaurus\n",
      "Gaalosaurus\n",
      "Troionosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.231165\n",
      "\n",
      "Phyusodon\n",
      "Lomaaerond\n",
      "Mytrodon\n",
      "Pgaagptok\n",
      "Yussangosaurus\n",
      "Gaaerrdcaptosaurus\n",
      "Trodomor\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 24.000355\n",
      "\n",
      "Onyusaurus\n",
      "Mica\n",
      "Myusodon\n",
      "Ola\n",
      "Yuspandosaurus\n",
      "Fa\n",
      "Trocheoraxaurucoonatocesaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.363365\n",
      "\n",
      "Phytosaurus\n",
      "Miecanosaurus\n",
      "Myuspenatoptor\n",
      "Pedahosaurus\n",
      "Yvqsator\n",
      "Gaaerosaurus\n",
      "Ustanesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.307418\n",
      "\n",
      "Onyushaphophyloruciomistarshyllangoteratops\n",
      "Loecalosaurus\n",
      "Lytrocidon\n",
      "Ojbaetrocerotops\n",
      "Yusteritongnortator\n",
      "Ecaerolechyluicius\n",
      "Ustboloravlorsaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.194156\n",
      "\n",
      "Lhustia\n",
      "Hica\n",
      "Hyusia\n",
      "Lacalrosaurus\n",
      "Yuspamasaurus\n",
      "Daados\n",
      "Tosaurus\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.740432\n",
      "\n",
      "Phyusaurus\n",
      "Loja\n",
      "Lytrosaurus\n",
      "Qecagosaurus\n",
      "Yusterasaurus\n",
      "Gaaesqragosaurus\n",
      "Uslapeosaurus\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.659786898248683"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dinos.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "data = [x.lower() for x in data]\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "\n",
    "model = CharacterLanguageModel()\n",
    "model.train(data, ix_to_char, char_to_ix, num_iterations=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "éœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼š\n",
    "\n",
    "1. æ¯æ¬¡è¿­ä»£ï¼ˆoptimizeï¼‰ä¹‹åï¼Œè¾“å‡ºçš„æœ€åä¸€ä¸ªaï¼Œ ä½œä¸ºä¸‹ä¸€æ¬¡optimizeçš„ a0ã€‚ å¾ªç¯ä½¿ç”¨ã€‚ è¿™ä¸ªä¹‹å‰æ²¡å‘ç°ã€‚ä¸çŸ¥é“åŸå› ï¼Ÿï¼Ÿ\n",
    "\n",
    "2. ä¸åŸç‰ˆæœ¬ç›¸æ¯”ï¼Œ åœ¨è¿­ä»£ 400å¤šæ¬¡ä¹‹åï¼Œ losså°±æœ‰ä¸€ç‚¹ç‚¹åå·®äº†ã€‚ ä¸çŸ¥é“å“ªé‡Œè®¡ç®—å¼€å§‹å‡ºç°åå·®ã€‚ä¸è¿‡å¥½åœ¨æ¯”åŸç‰ˆæœ¬çš„losså°ä¸€ç‚¹ã€‚è¾“å‡ºä¹Ÿæ˜¯æœ‰æ„ä¹‰çš„ã€‚\n",
    "\n",
    "è®¾è®¡æ€æƒ³ï¼š\n",
    "\n",
    "RNN cell ä¼šå­˜å‚¨ä¸€äº›çŠ¶æ€ï¼Œä½¿ç”¨ä¸€äº›å‚æ•°ã€‚ æ¯æ¬¡ FPçš„æ—¶å€™ï¼Œæ›´æ–°ä¸€äº›çŠ¶æ€ï¼Œè®¡ç®—BPçš„æ—¶å€™ï¼Œä¼šç”¨åˆ°è¿™äº›çŠ¶æ€ï¼Œç„¶åæ›´æ–°å‚æ•°ã€‚\n",
    "è®­ç»ƒçš„æ—¶å€™ï¼Œç”Ÿæˆä¸€ä¸²cellï¼Œ è®¡ç®—FPå’ŒBPã€‚\n",
    "\n",
    "Lojaï¼ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 å†™è¯—ï¼Œèå£«æ¯”äºš\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "corpus length: 94275\n",
      "number of unique characters in the corpus: 38\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: ':', 10: ';', 11: '?', 12: 'a', 13: 'b', 14: 'c', 15: 'd', 16: 'e', 17: 'f', 18: 'g', 19: 'h', 20: 'i', 21: 'j', 22: 'k', 23: 'l', 24: 'm', 25: 'n', 26: 'o', 27: 'p', 28: 'q', 29: 'r', 30: 's', 31: 't', 32: 'u', 33: 'v', 34: 'w', 35: 'x', 36: 'y', 37: 'z'}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import sys\n",
    "import io\n",
    "\n",
    "print(\"Loading text data...\")\n",
    "text = io.open('shakespeare.txt', encoding='utf-8').read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "Tx = 40\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print('number of unique characters in the corpus:', len(chars))\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_data(text, Tx = 40, stride = 3):\n",
    "    \"\"\"\n",
    "    Create a training set by scanning a window of size Tx over the text corpus, with stride 3.\n",
    "    \n",
    "    Arguments:\n",
    "    text -- string, corpus of Shakespearian poem\n",
    "    Tx -- sequence length, number of time-steps (or characters) in one training example\n",
    "    stride -- how much the window shifts itself while scanning\n",
    "    \n",
    "    Returns:\n",
    "    X -- list of training examples\n",
    "    Y -- list of training labels\n",
    "    \"\"\" \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(0, len(text) - Tx, stride):\n",
    "        X.append(text[i: i + Tx])\n",
    "        Y.append(text[i + Tx])    \n",
    "    print('number of training examples:', len(X))\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def vectorization(X, Y, n_x, char_indices, Tx = 40):\n",
    "    \"\"\"\n",
    "    Convert X and Y (lists) into arrays to be given to a recurrent neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- \n",
    "    Y -- \n",
    "    Tx -- integer, sequence length\n",
    "    \n",
    "    Returns:\n",
    "    x -- array of shape (m, Tx, len(chars))\n",
    "    y -- array of shape (m, len(chars))\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    y = np.zeros((m, n_x), dtype=np.bool)\n",
    "    for i, sentence in enumerate(X):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[Y[i]]] = 1\n",
    "        \n",
    "    return x, y \n",
    "\n",
    "print(\"Creating training set...\")\n",
    "X, Y = build_data(text, Tx, stride = 3)\n",
    "print(\"Vectorizing training set...\")\n",
    "x, y = vectorization(X, Y, n_x = len(chars), char_indices = char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model = load_model('models/model_shakespeare_kiank_350_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31412/31412 [==============================] - 97s 3ms/step - loss: 2.5651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11db2da90>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)    # softmax\n",
    "    probas = np.random.multinomial(1, preds, 1)    # æ·1æ¬¡ğŸ² ï¼Ÿ å‡ºç°å“ªä¸€é¢ã€‚ã€‚ã€‚ [0...1...0]\n",
    "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
    "    return out\n",
    "    #return np.argmax(probas)\n",
    "    \n",
    "def generate_output():\n",
    "    generated = ''\n",
    "    #sentence = text[start_index: start_index + Tx]\n",
    "    #sentence = '0'*Tx\n",
    "    usr_input = input(\"Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: \")\n",
    "    # zero pad the sentence to Tx characters.\n",
    "    sentence = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "    generated += usr_input \n",
    "\n",
    "    sys.stdout.write(\"\\n\\nHere is your poem: \\n\\n\") \n",
    "    sys.stdout.write(usr_input)\n",
    "    for i in range(400):\n",
    "\n",
    "        x_pred = np.zeros((1, Tx, len(chars)))\n",
    "\n",
    "        for t, char in enumerate(sentence):\n",
    "            if char != '0':\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature = 1.0)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if next_char == '\\n':\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: hello\n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "hellowe:\n",
      "thint no nhe cow let yet net thus widl cenn as ofe,\n",
      "the bibtt of my knew mate batuted his growl rier,\n",
      "which persed wimn dopr tom, thy wasterfoe,\n",
      "to hil crothes urufnes the riming stet dannter:\n",
      "what hin gray who braws a dacm tot con  now\n",
      "thought is fides vieture my hell yeur o thee.\n",
      "\n",
      "so thon that i pun gold thy mate woed one,\n",
      "whins the lich thy happlint who thees bid,\n",
      "thy sich basth hir sides b"
     ]
    }
   ],
   "source": [
    "# Run this cell to try with different inputs without having to re-train the model \n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "æ€»ç»“ä¸€ä¸‹ï¼š\n",
    "\n",
    "ç”¨çš„LSTM æ¨¡å‹ï¼Œ è¿˜æ˜¯ Character Lever Modelã€‚ ç”¨è‹¥å¹²å­—ç¬¦é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚  è®­ç»ƒçš„æ—¶å€™ç”¨40ä¸ªå­—ç¬¦é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚\n",
    "\n",
    "æ¯”è¾ƒç¥å¥‡çš„æ˜¯ï¼Œ é™¤äº†æ¨¡å‹ï¼Œæ²¡æœ‰ç»™å‡ºå…¶ä»–è§„åˆ™ï¼Œæ¯”å¦‚æ¢è¡Œã€','åé¢åŸºæœ¬ä¸Šéƒ½æ˜¯æ¢è¡Œã€‚ å­¦ä¹ çš„æ—¶å€™è‡ªå·±å°±ä¼šäº†ã€‚\n",
    "\n",
    "è¾“å‡ºçš„ç©æ„ä¹Ÿä¸çŸ¥æ‰€äº‘ã€‚"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
