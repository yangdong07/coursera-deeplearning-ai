{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN exercises\n",
    "\n",
    "\n",
    "目标：\n",
    "\n",
    "1. Character Language Model,  Basic RNN\n",
    "    1. word representation\n",
    "    2. character language model\n",
    "    3. sample\n",
    "    4. train\n",
    "    5. summary\n",
    "2. Writing like Shakespeare, LSTM RNN\n",
    "    1. Model\n",
    "    2. Train\n",
    "    3. Generate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 取名字\n",
    "\n",
    "给定一个name列表（训练集），训练RNN模型，得到一个 Character Language Model（概率模型）。 然后通过这个Model Sample出一些name。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# 数据集： list of names\n",
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "# 统计字符，字符字典\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "<img src=\"images/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> **Figure 1**: Recurrent Neural Network, similar to what you had built in the previous notebook \"Building a RNN - Step by Step\".  </center></caption>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 2**: In this picture, we assume the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step, and have the network then sample one character at a time. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnCell(object):\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.graidents = {}\n",
    "        self.x = None\n",
    "        self.ai = None\n",
    "        self.y = None\n",
    "        self.ao = None\n",
    "        self.dy = None\n",
    "        \n",
    "    def forward_pass(self, x, ai):\n",
    "        Wax, Waa, ba, Wya, by = [self.parameters[k] for k in ['Wax', 'Waa', 'ba', 'Wya', 'by']]\n",
    "        self.x = x\n",
    "        self.ai = ai\n",
    "        self.ao = np.tanh(np.dot(Wax, x) + np.dot(Waa, ai) + ba)\n",
    "        self.y = softmax(np.dot(Wya, self.ao) + by)\n",
    "        return self.ao\n",
    "    \n",
    "    def backward_pass(self, dao):\n",
    "        Wax, Waa, Wya = [self.parameters[k] for k in ['Wax', 'Waa', 'Wya']]\n",
    "        dy = self.dy\n",
    "        \n",
    "        dWya = np.dot(dy, self.ao.T)\n",
    "        dby = dy\n",
    "        da = dao + np.dot(Wya.T, dy)\n",
    "        dz = da * (1 - self.ao * self.ao)  # a = tanh(z), dz = da * (1 - a * a)\n",
    "        dWaa = np.dot(dz, self.ai.T)\n",
    "        dWax = np.dot(dz, self.x.T)\n",
    "        dba = dz\n",
    "        dai = np.dot(Waa.T, dz)\n",
    "        gradients = {'Wax': dWax, 'Waa': dWaa, 'ba': dba, 'Wya': dWya, 'by': dby}\n",
    "        return gradients, dai        \n",
    "\n",
    "\n",
    "class CharacterLanguageModel():\n",
    "    def __init__(self, n_a=50, vocab_size=27):\n",
    "        self.n_x = vocab_size\n",
    "        self.n_y = vocab_size   # T_y = T_x, n_y = n_x\n",
    "        self.n_a = n_a\n",
    "        self.initialize_parameters(n_a, vocab_size, vocab_size)\n",
    "        \n",
    "    def initialize_parameters(self, n_a, n_x, n_y, seed=1):\n",
    "        np.random.seed(seed)\n",
    "        Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "        Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "        Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "        b = np.zeros((n_a, 1)) # hidden bias\n",
    "        by = np.zeros((n_y, 1)) # output bias\n",
    "\n",
    "        self.parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": b,\"by\": by}\n",
    "        \n",
    "    def run_forward(self, X, a0):\n",
    "        T_x = len(X)\n",
    "        loss = 0.0\n",
    "        cells = []\n",
    "        \n",
    "        x = np.zeros((self.n_x, 1))\n",
    "        ai = np.copy(a0)\n",
    "        for t in range(T_x):\n",
    "            cell = RnnCell(self.parameters)\n",
    "            ai = cell.forward_pass(x, ai)\n",
    "            \n",
    "            # loss and dy\n",
    "            loss -= np.log(cell.y[X[t], 0])\n",
    "            cell.dy = cell.y.copy()\n",
    "            cell.dy[X[t]] -= 1\n",
    "            \n",
    "            cells.append(cell)\n",
    "            # for next\n",
    "            x = np.zeros((self.n_x, 1))\n",
    "            x[X[t]] = 1\n",
    "        \n",
    "#         print(cells[-1].ao[:5])\n",
    "        return loss, cells\n",
    "    \n",
    "    def run_backwards(self, cells):\n",
    "        dao = np.zeros((self.n_a, 1))\n",
    "        gradients = {}\n",
    "        for k in self.parameters:\n",
    "            gradients[k] = np.zeros_like(self.parameters[k])\n",
    "        for cell in reversed(cells):\n",
    "            grad, dao = cell.backward_pass(dao) \n",
    "            for k in grad:\n",
    "                gradients[k] += grad[k]\n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients, learning_rate):        \n",
    "        # clip\n",
    "        for gradient in gradients.values():\n",
    "            np.clip(gradient, -1, 1, out=gradient)\n",
    "            \n",
    "        for k in self.parameters:\n",
    "            self.parameters[k] += -learning_rate * gradients[k]\n",
    "     \n",
    "    def optimize(self, X, a0, learning_rate=0.01):\n",
    "        # X: list of character index        \n",
    "        T_x = len(X)\n",
    "        loss, cells = self.run_forward(X, a0)\n",
    "        gradients = self.run_backwards(cells)\n",
    "        self.update_parameters(gradients, learning_rate)\n",
    "        return loss, gradients, cells[-1].ao\n",
    "        \n",
    "    def train(self, data, ix_to_char, char_to_ix, num_iterations=35000, learning_rate=0.01):\n",
    "        # data:  list of lines, from f.readlines()\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(data)\n",
    "        \n",
    "        # Optimization loop\n",
    "        loss = get_initial_loss(self.n_x, 7) #??\n",
    "        a0 = np.zeros((self.n_a, 1))\n",
    "        for j in range(num_iterations):\n",
    "            index = j % len(data)\n",
    "            X = [char_to_ix[ch] for ch in data[index]]\n",
    "            \n",
    "            curr_loss, gradients, a0 = self.optimize(X, a0, learning_rate=0.01)\n",
    "            loss = smooth(loss, curr_loss)\n",
    "#             if j % 100 == 0:\n",
    "#                 print(curr_loss, loss)      \n",
    "            if j % 2000 == 0:\n",
    "                print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "                seed = 0\n",
    "                for name in range(7):\n",
    "\n",
    "                    # Sample indices and print them\n",
    "                    sampled_indices = self.sample(char_to_ix['\\n'], seed=seed)\n",
    "                    print_sample(sampled_indices, ix_to_char)\n",
    "                    seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "                print('\\n')\n",
    "        return curr_loss\n",
    "    \n",
    "    def sample(self, eos=0, max_len=50, seed=1):\n",
    "        \n",
    "        Wax, Waa, ba, Wya, by = [self.parameters[k] for k in ['Wax', 'Waa', 'ba', 'Wya', 'by']]\n",
    "        n_x, n_a = self.n_x, self.n_a\n",
    "        \n",
    "        indices = []\n",
    "        idx = -1 \n",
    "        counter = 0\n",
    "        \n",
    "        # 1. init x and a_prev\n",
    "        x = np.zeros((n_x, 1))\n",
    "        ai = np.zeros((n_a, 1))\n",
    "        while (idx != eos and counter != max_len):\n",
    "            # 2. calculate\n",
    "            a = np.tanh(np.dot(Waa, ai) + np.dot(Wax, x) + ba)\n",
    "            y = softmax(np.dot(Wya, a) + by)\n",
    "\n",
    "            # 3. Sample the index of a character within the vocabulary from the probability distribution y\n",
    "            np.random.seed(counter + seed) \n",
    "            idx = np.random.choice(n_x, p = y.ravel())\n",
    "            indices.append(idx)\n",
    "            \n",
    "            # 4. next x = y : x<t> = y<t-1>, x : one-hot encoding\n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            \n",
    "            seed += 1\n",
    "            counter +=1\n",
    "            ai = a\n",
    "\n",
    "        if (counter == max_len):\n",
    "            indices.append(eos)\n",
    "        return indices\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "aachenosaurus\n",
      "\n",
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 27.950466\n",
      "\n",
      "Mhytrpdmeromxgortariontoclusuonancesatlasalepdtonp\n",
      "Hledalpsamantisaurus\n",
      "Iwtrpdlerndxhortarinesganusvkecielulen\n",
      "Macalpsamantisaurus\n",
      "Ytrpckgoraurus\n",
      "A\n",
      "Troligoraurus\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.894695\n",
      "\n",
      "Onytos\n",
      "Kledalosaurus\n",
      "Lytosaurus\n",
      "Oma\n",
      "Xtrolonmkaveros\n",
      "Cabasemachus\n",
      "Toraraurus\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.697856\n",
      "\n",
      "Rixtosaurus\n",
      "Nneeaitos\n",
      "Nytosaurus\n",
      "Racalosaurus\n",
      "Xtroionosaurus\n",
      "Gaalosaurus\n",
      "Troionosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.231165\n",
      "\n",
      "Phyusodon\n",
      "Lomaaerond\n",
      "Mytrodon\n",
      "Pgaagptok\n",
      "Yussangosaurus\n",
      "Gaaerrdcaptosaurus\n",
      "Trodomor\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 24.000355\n",
      "\n",
      "Onyusaurus\n",
      "Mica\n",
      "Myusodon\n",
      "Ola\n",
      "Yuspandosaurus\n",
      "Fa\n",
      "Trocheoraxaurucoonatocesaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.363365\n",
      "\n",
      "Phytosaurus\n",
      "Miecanosaurus\n",
      "Myuspenatoptor\n",
      "Pedahosaurus\n",
      "Yvqsator\n",
      "Gaaerosaurus\n",
      "Ustanesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.307418\n",
      "\n",
      "Onyushaphophyloruciomistarshyllangoteratops\n",
      "Loecalosaurus\n",
      "Lytrocidon\n",
      "Ojbaetrocerotops\n",
      "Yusteritongnortator\n",
      "Ecaerolechyluicius\n",
      "Ustboloravlorsaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.194156\n",
      "\n",
      "Lhustia\n",
      "Hica\n",
      "Hyusia\n",
      "Lacalrosaurus\n",
      "Yuspamasaurus\n",
      "Daados\n",
      "Tosaurus\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.740432\n",
      "\n",
      "Phyusaurus\n",
      "Loja\n",
      "Lytrosaurus\n",
      "Qecagosaurus\n",
      "Yusterasaurus\n",
      "Gaaesqragosaurus\n",
      "Uslapeosaurus\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.659786898248683"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dinos.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "data = [x.lower() for x in data]\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "\n",
    "model = CharacterLanguageModel()\n",
    "model.train(data, ix_to_char, char_to_ix, num_iterations=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "需要注意的地方：\n",
    "\n",
    "1. 每次迭代（optimize）之后，输出的最后一个a， 作为下一次optimize的 a0。 循环使用。 这个之前没发现。不知道原因？？\n",
    "\n",
    "2. 与原版本相比， 在迭代 400多次之后， loss就有一点点偏差了。 不知道哪里计算开始出现偏差。不过好在比原版本的loss小一点。输出也是有意义的。\n",
    "\n",
    "设计思想：\n",
    "\n",
    "RNN cell 会存储一些状态，使用一些参数。 每次 FP的时候，更新一些状态，计算BP的时候，会用到这些状态，然后更新参数。\n",
    "训练的时候，生成一串cell， 计算FP和BP。\n",
    "\n",
    "Loja！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 写诗，莎士比亚\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "corpus length: 94275\n",
      "number of unique characters in the corpus: 38\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: ':', 10: ';', 11: '?', 12: 'a', 13: 'b', 14: 'c', 15: 'd', 16: 'e', 17: 'f', 18: 'g', 19: 'h', 20: 'i', 21: 'j', 22: 'k', 23: 'l', 24: 'm', 25: 'n', 26: 'o', 27: 'p', 28: 'q', 29: 'r', 30: 's', 31: 't', 32: 'u', 33: 'v', 34: 'w', 35: 'x', 36: 'y', 37: 'z'}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import sys\n",
    "import io\n",
    "\n",
    "print(\"Loading text data...\")\n",
    "text = io.open('shakespeare.txt', encoding='utf-8').read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "Tx = 40\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print('number of unique characters in the corpus:', len(chars))\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_data(text, Tx = 40, stride = 3):\n",
    "    \"\"\"\n",
    "    Create a training set by scanning a window of size Tx over the text corpus, with stride 3.\n",
    "    \n",
    "    Arguments:\n",
    "    text -- string, corpus of Shakespearian poem\n",
    "    Tx -- sequence length, number of time-steps (or characters) in one training example\n",
    "    stride -- how much the window shifts itself while scanning\n",
    "    \n",
    "    Returns:\n",
    "    X -- list of training examples\n",
    "    Y -- list of training labels\n",
    "    \"\"\" \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(0, len(text) - Tx, stride):\n",
    "        X.append(text[i: i + Tx])\n",
    "        Y.append(text[i + Tx])    \n",
    "    print('number of training examples:', len(X))\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def vectorization(X, Y, n_x, char_indices, Tx = 40):\n",
    "    \"\"\"\n",
    "    Convert X and Y (lists) into arrays to be given to a recurrent neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- \n",
    "    Y -- \n",
    "    Tx -- integer, sequence length\n",
    "    \n",
    "    Returns:\n",
    "    x -- array of shape (m, Tx, len(chars))\n",
    "    y -- array of shape (m, len(chars))\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    y = np.zeros((m, n_x), dtype=np.bool)\n",
    "    for i, sentence in enumerate(X):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[Y[i]]] = 1\n",
    "        \n",
    "    return x, y \n",
    "\n",
    "print(\"Creating training set...\")\n",
    "X, Y = build_data(text, Tx, stride = 3)\n",
    "print(\"Vectorizing training set...\")\n",
    "x, y = vectorization(X, Y, n_x = len(chars), char_indices = char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model = load_model('models/model_shakespeare_kiank_350_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31412/31412 [==============================] - 97s 3ms/step - loss: 2.5651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11db2da90>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)    # softmax\n",
    "    probas = np.random.multinomial(1, preds, 1)    # 掷1次🎲 ？ 出现哪一面。。。 [0...1...0]\n",
    "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
    "    return out\n",
    "    #return np.argmax(probas)\n",
    "    \n",
    "def generate_output():\n",
    "    generated = ''\n",
    "    #sentence = text[start_index: start_index + Tx]\n",
    "    #sentence = '0'*Tx\n",
    "    usr_input = input(\"Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: \")\n",
    "    # zero pad the sentence to Tx characters.\n",
    "    sentence = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "    generated += usr_input \n",
    "\n",
    "    sys.stdout.write(\"\\n\\nHere is your poem: \\n\\n\") \n",
    "    sys.stdout.write(usr_input)\n",
    "    for i in range(400):\n",
    "\n",
    "        x_pred = np.zeros((1, Tx, len(chars)))\n",
    "\n",
    "        for t, char in enumerate(sentence):\n",
    "            if char != '0':\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature = 1.0)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if next_char == '\\n':\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: hello\n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "hellowe:\n",
      "thint no nhe cow let yet net thus widl cenn as ofe,\n",
      "the bibtt of my knew mate batuted his growl rier,\n",
      "which persed wimn dopr tom, thy wasterfoe,\n",
      "to hil crothes urufnes the riming stet dannter:\n",
      "what hin gray who braws a dacm tot con  now\n",
      "thought is fides vieture my hell yeur o thee.\n",
      "\n",
      "so thon that i pun gold thy mate woed one,\n",
      "whins the lich thy happlint who thees bid,\n",
      "thy sich basth hir sides b"
     ]
    }
   ],
   "source": [
    "# Run this cell to try with different inputs without having to re-train the model \n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "总结一下：\n",
    "\n",
    "用的LSTM 模型， 还是 Character Lever Model。 用若干字符预测下一个字符。  训练的时候用40个字符预测下一个字符。\n",
    "\n",
    "比较神奇的是， 除了模型，没有给出其他规则，比如换行、','后面基本上都是换行。 学习的时候自己就会了。\n",
    "\n",
    "输出的玩意也不知所云。"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
