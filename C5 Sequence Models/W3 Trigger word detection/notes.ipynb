{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger Word Detection\n",
    "\n",
    "目标：语音识别之关键词触发\n",
    "\n",
    "1. 理解一下音频文件wav，以及音频分析（spectrogram）\n",
    "2. 理解一下如何语音合成，以生成训练数据： Trigger Word: \"Activate\"、Negative 以及 Background\n",
    "3. 构造一个模型，包括 Conv1D 以及 RNN（GRU），输出 探测信号。 \n",
    "4. 使用已经训练好的模型，训练已经准备好的数据。。。\n",
    "5. 使用预测结果，生成一个声音文件，听听触发的效果\n",
    "\n",
    "\n",
    "学到：\n",
    "1. 如何使用语音合成，生成训练数据\n",
    "2. 构建一个模型，探测关键词\n",
    "\n",
    "不明白的地方还有：\n",
    "\n",
    "1. TimeDistributed 的作用\n",
    "2. 下载的数据，训练产生了负面效果，不知道原理\n",
    "3. 还是需要实际操作一遍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 wav文件以及spectogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"./raw_data/activates/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/negatives/4.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/backgrounds/1.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键词是 “activate”，其他是干扰和背景音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "rate, data = wavfile.read(\"audio_examples/example_train.wav\")\n",
    "# print(\"Time steps in audio recording before spectrogram\", data[:,0].shape)\n",
    "# print(\"Time steps in input after spectrogram\", x.shape)\n",
    "\n",
    "print(rate)\n",
    "print(data.shape)\n",
    "print(data[500:510])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "声音信号是用  44.1kHz采样，  10s的声音文件，有 44,100 * 10 = 441,000 个值，这个没问题\n",
    "\n",
    "使用频谱分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_spectrogram(rate, data):\n",
    "    nfft = 200 # Length of each window segment\n",
    "    fs = 8000  # Sampling frequencies\n",
    "    noverlap = 120 # Overlap between windows\n",
    "    nchannels = data.ndim\n",
    "    if nchannels == 1:\n",
    "        pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap = noverlap)\n",
    "    elif nchannels == 2:\n",
    "        pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n",
    "    return pxx\n",
    "\n",
    "x = graph_spectrogram(rate, data)\n",
    "\n",
    "print(x.shape)\n",
    "print(x[20:25, 1000:1005])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然不懂上面在干什么，大概分析一些东西：\n",
    "\n",
    "`nfft=200` 决定了分析的时间段（窗口）， `noverlap = 120` 表示重叠区域， 这样分析的步长实际是 80， 会得到： $(441000 - 200) / 80 + 1 = 5511 $\n",
    "\n",
    "这个公式是： $ \\frac{ N - window}{step} + 1 $， 为什么 $+1$ 是因为先去掉第一个窗口，此后每步对应一个窗口，共有 $ \\frac{ N - window}{step} $ 个窗口， 加上第一个窗口，就是总采样数。 其实对应时间周，表示 $Tx$\n",
    "\n",
    "101 这个数值不知道从哪里来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tx = 5511 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram\n",
    "Ty = 1375 # The number of time steps in the output of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ty = 1375 这个数，是打算对 1D的音频数据（ 5511个点，101个频率，相当于 5511的1D数据， 101个channel）做一次 Conv1D， 使用 filter size 为15， stride 为4， 这样就得到 $ (5511 - 15) / 4 + 1 = 1375 $ ，得到 1375 个点， 196个channel，即 Ty = 1375, vector 大小为 196\n",
    "\n",
    "因为统一用10s的语音文件， 有以下数据：\n",
    "\n",
    "- $441000$ (raw audio) ， 每step表示： 0.000023s \n",
    "- $5511 = T_x$ (spectrogram output steps) ，每step表示： 0.0018s \n",
    "- $10000$ (used by the `pydub` module to synthesize audio)， 每step表示： 1ms\n",
    "- $1375 = T_y$ (RNN GRU 输出步数).  每step表示： 0.0072s\n",
    "\n",
    "第三个数值10000， 用在语音合成里面： 将一段10s的语音表示为10000个step，每个step表示1ms语音。 这样在背景音里随机插入一段关键词（activate）或干扰词（negative），方便使用index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 语音合成，生成训练集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def load_raw_audio():\n",
    "    activates = []\n",
    "    backgrounds = []\n",
    "    negatives = []\n",
    "    for filename in os.listdir(\"./raw_data/activates\"):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            activate = AudioSegment.from_wav(\"./raw_data/activates/\"+filename)\n",
    "            activates.append(activate)\n",
    "    for filename in os.listdir(\"./raw_data/backgrounds\"):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            background = AudioSegment.from_wav(\"./raw_data/backgrounds/\"+filename)\n",
    "            backgrounds.append(background)\n",
    "    for filename in os.listdir(\"./raw_data/negatives\"):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            negative = AudioSegment.from_wav(\"./raw_data/negatives/\"+filename)\n",
    "            negatives.append(negative)\n",
    "    return activates, negatives, backgrounds\n",
    "\n",
    "# Load audio segments using pydub \n",
    "activates, negatives, backgrounds = load_raw_audio()\n",
    "\n",
    "print(\"background len: \" + str(len(backgrounds[0])))    # Should be 10,000, since it is a 10 sec clip\n",
    "print(\"activate[0] len: \" + str(len(activates[0])))     # Maybe around 1000, since an \"activate\" audio clip is usually around 1 sec (but varies a lot)\n",
    "print(\"activate[1] len: \" + str(len(activates[1])))     # Different \"activate\" clips can have different lengths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "背景音统一长度 10000， 关键词和干扰词长度较小。\n",
    "\n",
    "** 生成训练集的方法：合成语音，在背景音中随机添加关键词和干扰词 **\n",
    "\n",
    "这里注意几点：\n",
    "1. 背景音统一长度 10000，表示10s\n",
    "2. 关键词和干扰词放进去的时候， 互相不可重叠。（？为啥？干扰也可以作为背景音）\n",
    "3. 输出的标签：当关键词结束的时候，对应位置，输出**50个1**，其他位置均为0，如下图。注意这里标签长度Ty = 1375\n",
    "\n",
    "<img src=\"images/label_diagram.png\" style=\"width:500px;height:200px;\">\n",
    "<center> **Figure 2** </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 根据片段长度，随机选取一个插入位置\n",
    "def get_random_time_segment(segment_ms, total_ms=10000):    \n",
    "    segment_start = np.random.randint(low=0, high=total_ms-segment_ms)   # Make sure segment doesn't run past the 10sec background \n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    return (segment_start, segment_end)\n",
    "\n",
    "\n",
    "# 判断是否有重叠\n",
    "def is_overlapping(sa, sb):\n",
    "    return not (sa[0] > sb[1] or sa[1] < sb[0])\n",
    "\n",
    "def is_invalid_segment(segment, previous_segments):\n",
    "    for s in previous_segments:\n",
    "        if is_overlapping(segment, s):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# 插入一个声音片段\n",
    "def insert_audio_clip(background, audio_clip, previous_segments):\n",
    "    # background, audio_clip:  AudioSegment object\n",
    "    # previous_segments:  list\n",
    "    \n",
    "    segment_ms = len(audio_clip)\n",
    "    segment_time = get_random_time_segment(segment_ms)\n",
    "    \n",
    "    while is_invalid_segment(segment_time, previous_segments):\n",
    "        segment_time = get_random_time_segment(segment_ms)\n",
    "        \n",
    "    previous_segments.append(segment_time)\n",
    "    \n",
    "    # 插入\n",
    "    new_background = background.overlay(audio_clip, position = segment_time[0])\n",
    "    \n",
    "    return new_background, segment_time\n",
    "\n",
    "\n",
    "# 在输出标签的对应位置 插入 50个1\n",
    "def insert_ones(y, segment_end_ms, total_ms=10000.0):\n",
    "    # y.shape = (1, Ty)\n",
    "    Ty = y.shape[1]\n",
    "    segment_end_y = int(segment_end_ms * Ty / total_ms)\n",
    "    start = segment_end_y + 1\n",
    "    y[0, start:start+50] = 1    \n",
    "    return y\n",
    "\n",
    "\n",
    "# 创建训练数据集，以及标签\n",
    "def create_training_example(background, activates, negatives): \n",
    "    \n",
    "    np.random.seed(18)\n",
    "    \n",
    "    # Make background quieter\n",
    "    background = background - 20\n",
    "\n",
    "    y = np.zeros((1, Ty))\n",
    "    previous_segments = []\n",
    "\n",
    "    # 随机选取 0~4 个 avtivate 声音片段，插入background\n",
    "    number_of_activates = np.random.randint(0, 5)\n",
    "    random_indices = np.random.randint(len(activates), size=number_of_activates)\n",
    "    \n",
    "    for i in random_indices:\n",
    "        activate = activates[i]\n",
    "        # Insert the audio clip on the background\n",
    "        background, segment_time = insert_audio_clip(background, activate, previous_segments)\n",
    "        # Retrieve segment_start and segment_end from segment_time\n",
    "        segment_start, segment_end = segment_time\n",
    "        # Insert labels in \"y\"\n",
    "        y = insert_ones(y, segment_end)\n",
    "        \n",
    "    \n",
    "    # 随机选取 0~2 个 negative 声音片段，插入background\n",
    "    number_of_negatives = np.random.randint(0, 3)\n",
    "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
    "    \n",
    "    for i in random_indices:\n",
    "        negative = negatives[i]\n",
    "        # Insert the audio clip on the background \n",
    "        background, _ = insert_audio_clip(background, negative, previous_segments)\n",
    "    \n",
    "    # Standardize the volume of the audio clip ，不懂\n",
    "    background = background.apply_gain(-20.0 - background.dBFS)\n",
    "\n",
    "    # Export new training example \n",
    "    wav_filename = \"train.wav\"\n",
    "    file_handle = background.export(wav_filename, format=\"wav\")\n",
    "    print(\"File (train.wav) was saved in your directory.\")\n",
    "    \n",
    "    # Get and plot spectrogram of the new recording (background with superposition of positive and negatives)\n",
    "    rate, data = wavfile.read(wav_filename)\n",
    "    x = graph_spectrogram(rate, data)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_training_example(backgrounds[0], activates, negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/train_reference.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:600px;height:600px;\">\n",
    "<center> **Figure 3** </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里：\n",
    "\n",
    "1. 一层 Conv1D，filters=196， kernel size是15， stride是4，这样将 5511 长度数据转成 1375 长度，作为 Tx（上面计算过）。 每个vector 维度为196\n",
    "2. 一层 GRU， hidden states 有128个\n",
    "3. 又一层 GRU， hidden states 有128个\n",
    "4. [TimeDistributed](https://keras.io/layers/wrappers/)，这个大概起一个复制的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    X_input = Input(shape = input_shape)\n",
    "    \n",
    "    # Step 1: CONV layer\n",
    "    X = Conv1D(196, 15, strides=4)(X_input)        # CONV1D\n",
    "    X = BatchNormalization()(X)              # Batch normalization\n",
    "    X = Activation('relu')(X)                # ReLu activation\n",
    "    X = Dropout(0.8)(X)                      # dropout (use 0.8)\n",
    "\n",
    "    # Step 2: First GRU Layer\n",
    "    X = GRU(units = 128, return_sequences = True)(X)    # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    X = BatchNormalization()(X)                         # Batch normalization\n",
    "    \n",
    "    # Step 3: Second GRU Layer \n",
    "    X = GRU(units = 128, return_sequences = True)(X)    # GRU (use 128 units and return the sequences)\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    X = BatchNormalization()(X)                         # Batch normalization\n",
    "    X = Dropout(0.8)(X)                                 # dropout (use 0.8)\n",
    "    \n",
    "    # Step 4: Time-distributed dense layer \n",
    "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X) # time distributed  (sigmoid)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model(input_shape = (Tx, n_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里最后一层： `TimeDist (None, 1375, 1)` 参数有 129个， 其实就是1个 `W.shape = (128, 1)` 和一个 `b` 。所以 TimeDistributed 使 Dense 共享了这些参数？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 训练数据\n",
    "\n",
    "这里用了已经训练好的模型，训练已经准备好的数据。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('./models/tr_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load preprocessed training examples\n",
    "X = np.load(\"./data/X.npy\")\n",
    "Y = np.load(\"./data/Y.npy\")\n",
    "X_dev = np.load(\"./data/X_dev.npy\")\n",
    "Y_dev = np.load(\"./data/Y_dev.npy\")\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_dev, Y_dev, batch_size = 5, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_dev, Y_dev)\n",
    "print(\"Dev set accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "这个使用 accuracy评分。 实际上accuracy不是好的metric。 因为输出大部分都是0，全部预测为0，也能到 90%的准确率。\n",
    "\n",
    "**从课程里面下载的数据， X，Y，在训练一个epoch反而使模型变差了。 不用训练反而能正确输出**，怀疑下载的数据有点问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 预测及输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 预测\n",
    "def detect_triggerword(filename):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    \n",
    "    rate, data = wavfile.read(filename)\n",
    "    x = graph_spectrogram(rate, data)\n",
    "    # the spectogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
    "    x  = x.swapaxes(0, 1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = model.predict(x)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(predictions[0,:,0])\n",
    "    plt.ylabel('probability')\n",
    "    plt.show()\n",
    "    return predictions\n",
    "\n",
    "# 合成\n",
    "chime_file = \"audio_examples/chime.wav\"\n",
    "def chime_on_activate(filename, predictions, threshold):\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(chime_file)\n",
    "    Ty = predictions.shape[1]\n",
    "    # Step 1: Initialize the number of consecutive output steps to 0\n",
    "    consecutive_timesteps = 0\n",
    "    # Step 2: Loop over the output steps in the y\n",
    "    for i in range(Ty):\n",
    "        # Step 3: Increment consecutive output steps\n",
    "        consecutive_timesteps += 1\n",
    "        # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n",
    "        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n",
    "            # Step 5: Superpose audio and background using pydub\n",
    "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n",
    "            # Step 6: Reset consecutive output steps to 0\n",
    "            consecutive_timesteps = 0\n",
    "        \n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = \"./raw_data/dev/1.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename  = \"./raw_data/dev/2.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the audio to the correct format\n",
    "def preprocess_audio(filename):\n",
    "    # Trim or pad audio segment to 10000ms\n",
    "    padding = AudioSegment.silent(duration=10000)\n",
    "    segment = AudioSegment.from_wav(filename)[:10000]\n",
    "    segment = padding.overlay(segment)\n",
    "    # Set frame rate to 44100\n",
    "    segment = segment.set_frame_rate(44100)\n",
    "    # Export as wav\n",
    "    segment.export(filename, format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've uploaded your audio file to Coursera, put the path to your file in the variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "your_filename = \"audio_examples/my_audio.wav\"\n",
    "\n",
    "preprocess_audio(your_filename)\n",
    "IPython.display.Audio(your_filename) # listen to the audio you uploaded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the model to predict when you say activate in the 10 second audio clip, and trigger a chime. If beeps are not being added appropriately, try to adjust the chime_threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime_threshold = 0.5\n",
    "prediction = detect_triggerword(your_filename)\n",
    "chime_on_activate(your_filename, prediction, chime_threshold)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "rSupZ",
   "launcher_item_id": "cvGhe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
